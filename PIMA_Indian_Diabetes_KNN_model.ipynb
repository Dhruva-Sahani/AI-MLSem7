{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "12454ce8-38b2-418e-98ef-69f0a2e96e19",
    "_uuid": "e72308cb607cb6d768e7d27fccfbe2a69281874e"
   },
   "source": [
    "# Prediction of Diabetes using KNN algorithm in PIMA Indian Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "059208d1-5542-4d80-9ee2-cee9a4391e78",
    "_uuid": "333dc72902f9a7b163226cb187cf989768665c43",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the necessary python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "0d788097-f6b8-4fca-99b4-f5102d1bbade",
    "_uuid": "d15b5491570a8ec6188b77240ab334f9b68f2655"
   },
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "df = pd.read_csv('../input/diabetes.csv')\n",
    "\n",
    "#Print the first 5 rows of the dataframe.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "766ae7f8-b08c-4ca7-bc1e-7e0401ac7fce",
    "_uuid": "25e59b9710940c6b4d86dc6e1487ce41dbf284e3"
   },
   "outputs": [],
   "source": [
    "#Let's observe the shape of the dataframe.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d1623be8-e5c9-4359-9747-bcaa469429ff",
    "_uuid": "765f95aef62cf293597ae9fc5c2a6063d1e83222"
   },
   "source": [
    "As observed above we have 768 rows and 9 columns. The first 8 columns represent the features and the last column represent the target/label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "a405c050-6a75-4c4a-803f-86db6152d796",
    "_uuid": "5b6e02cc6d372bd5c3b646959fdf3db8c0e0a881",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's create numpy arrays for features and target\n",
    "X = df.drop('Outcome',axis=1).values\n",
    "y = df['Outcome'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "18477358-3b6a-46e2-8b32-67dfc3150e53",
    "_uuid": "ce8256ba90e53d72f32d075e9beca80a32b4f14c"
   },
   "source": [
    "Let's split the data randomly into training and test set. \n",
    "\n",
    "We will fit/train a classifier on the training set and make predictions on the test set. Then we will compare the predictions with the known labels.\n",
    "\n",
    "Scikit-learn provides facility to split data into train and test set using train_test_split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "c674227c-f930-4ae0-a58a-c3f94c06c7d7",
    "_uuid": "892a7dac3f31c5d7ef1506933102ebdec2605ce0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92e7ad71-2384-4f93-93ef-e70476fbbe9f",
    "_uuid": "a4965dba9f73aa19400a4b767be4ce9bdb0426b2"
   },
   "source": [
    "It is a best practice to perform our split in such a way that out split reflects the labels in the data. We want labels to be split in train and test set as they are in the original dataset. So we use the stratify argument.\n",
    "\n",
    "Also we create a test set of size of about 40% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "380309a4-ecaa-43e6-be63-f0e657e5838b",
    "_uuid": "d90e61834caa0b28f7585e4322b49c1c2780d607",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f47ce31a-cd75-4294-9309-ede79264b977",
    "_uuid": "30d4cf1dd61428d69c95ca7dbbd2479c698275a1"
   },
   "source": [
    "Now we create a KNN algorithm for the data\n",
    "\n",
    "First let us first observe the accuracies for different values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "ca8346d5-a9a4-4728-8b24-253de0d9a31c",
    "_uuid": "745e27e18bf047cd2c71e014aebd324b306de5b5"
   },
   "outputs": [],
   "source": [
    "#import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Setup arrays to store training and test accuracies\n",
    "neighbors = np.arange(1,9)\n",
    "train_accuracy =np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "for i,k in enumerate(neighbors):\n",
    "    #Setup a knn classifier with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    #Fit the model\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the test set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "354c71d7-25f6-4862-9e5a-c390341c858b",
    "_uuid": "6e40ab5bc997d0f450215f65344bb79b4cf3896d"
   },
   "outputs": [],
   "source": [
    "#Generate plot\n",
    "plt.title('k-NN Varying number of neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label='Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label='Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b9ef2f66-9994-4a36-bbf3-1a4e5aadfc41",
    "_uuid": "f76fa52e8fab081315cf6a9aef45b6181e1593f5"
   },
   "source": [
    "We can observe above that we get maximum testing accuracy for k=7. So lets create a KNeighborsClassifier with number of neighbors as 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "d8cd01d6-f27a-43af-9ccf-4acb9ac351d7",
    "_uuid": "fd50a5bbfacbb723e2a27ed8cc2dd7de808875b6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setup a knn classifier with k neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "d9f66590-30d7-41c3-96f1-0d8abf6b759c",
    "_uuid": "927ab01dc02b5ccf1cc3a008b7d103d5f195daa5"
   },
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "3b16c03f-8619-4842-843c-5134b4cc49cb",
    "_uuid": "b4b0ae189ab063a00d5dfdef2847101a206ffa7c"
   },
   "outputs": [],
   "source": [
    "#Get accuracy. Note: In case of classification algorithms score method represents accuracy.\n",
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "daf29d5a-b7c1-4a8b-8492-72ddce1ca359",
    "_uuid": "380c3b4a0a147f6d89cccfc20d313cfaec6e4c8e"
   },
   "source": [
    "**Confusion Matrix**\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. Scikit-learn provides facility to calculate confusion matrix using the confusion_matrix method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "02c947c0-852b-4f19-8555-2e7116c47e85",
    "_uuid": "cdc17629041fc70eb5fd00685259d3aca24b8200",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "82e97b52f13616c7fefdb86902d7f2e1a3b52080",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let us get the predictions using the classifier we had fit above\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "215866ca-a484-4039-beb4-f18cdb3cf7f7",
    "_uuid": "ab884d77ed4789dcf33f0fe7d69c8705fc2273be"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dff9ee6e-5e97-4196-8b3a-ee5b199b3ba5",
    "_uuid": "56437155de90f62a1d550e5acfac3e1f9e2d1f01"
   },
   "source": [
    "Considering confusion matrix above:\n",
    "\n",
    "True negative = 165\n",
    "\n",
    "False positive = 36\n",
    "\n",
    "True postive = 60\n",
    "\n",
    "Fasle negative = 47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c1eba0d-4bac-4e6f-8820-e81d0564b9e9",
    "_uuid": "d9c67799afd1f26f7a80bd0d8038ad85a76dd66f"
   },
   "source": [
    "Confusion matrix can also be obtained using crosstab method of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "c9c26aea-37b5-42a2-b010-1e11f95c1fd9",
    "_uuid": "27bba67de6e6bd96cbe4a1420a8f864e5741d595"
   },
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ded13047fdabdd96d9e765b69a88638d80226c10"
   },
   "source": [
    "**Classification Report**\n",
    "\n",
    "Another important report is the Classification report. It is a text summary of the precision, recall, F1 score for each class. Scikit-learn provides facility to calculate Classification report using the classification_report method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "d9458cd7-b6fb-4268-a32a-8c87aa884c6c",
    "_uuid": "fe1119d6f66d5d8777e19ffd6b6237af30f07786",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import classification_report\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "154b90c999ba402d96bffaf0264ad467a05c10ae"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d01de7974804a39a757cff77debb7d6ce369977d"
   },
   "source": [
    "**ROC (Reciever Operating Charecteristic) curve**\n",
    "\n",
    "It is a plot of the true positive rate against the false positive rate for the different possible cutpoints of a diagnostic test.\n",
    "\n",
    "An ROC curve demonstrates several things:\n",
    "\n",
    "1) It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n",
    "\n",
    "2) The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n",
    "\n",
    "3)The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.\n",
    "\n",
    "4) The area under the curve is a measure of test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "09246b4a0fd2661fe961fee3e9d81535c0f8cde4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_proba = knn.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "57054382021f13117bff46195a14935f5a35c758",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "9b060d55660a7efd5d88dd3f6e9a67dbfcd1d75d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_uuid": "c799435270fca23e7a448be35c2dab20381a3796"
   },
   "outputs": [],
   "source": [
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.plot(fpr,tpr, label='Knn')\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('Knn(n_neighbors=7) ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "17143ca1a7fa550573b848e747fec666a2cc7355"
   },
   "outputs": [],
   "source": [
    "#Area under ROC curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test,y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec869f5802897e0eb39425bcb987d930078f0bf0"
   },
   "source": [
    "**Cross Validation**\n",
    "\n",
    "Now before getting into the details of Hyperparamter tuning, let us understand the concept of Cross validation.\n",
    "\n",
    "The trained model's performance is dependent on way the data is split. It might not representative of the model’s ability to generalize.\n",
    "\n",
    "The solution is cross validation.\n",
    "\n",
    "Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a test set to evaluate it. \n",
    "\n",
    "In k-fold cross-validation, the original sample is randomly partitioned into k equal size subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k-1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation. The advantage of this method is that all observations are used for both training and validation, and each observation is used for validation exactly once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06fc8b3adb267046d44b472d1c413707ce7e9964"
   },
   "source": [
    "**Hyperparameter tuning**\n",
    "\n",
    "The value of k (i.e 7) we selected above was selected by observing the curve of accuracy vs number of neighbors. This is a primitive way of hyperparameter tuning. \n",
    "\n",
    "There is a better way of doing it which involves:\n",
    "\n",
    "1) Trying a bunch of different hyperparameter values\n",
    "\n",
    "2) Fitting all of them separately\n",
    "\n",
    "3) Checking how well each performs\n",
    "\n",
    "4) Choosing the best performing one\n",
    "\n",
    "5) Using cross-validation every time\n",
    "\n",
    "Scikit-learn provides a simple way of achieving this using GridSearchCV i.e Grid Search cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "751e6a83ecd529354e07e237ab5596b27f9061c0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "e0c8e768a5912accaa8b9fd8dd4d8f6654a12fb5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In case of classifier like knn the parameter to be tuned is n_neighbors\n",
    "param_grid = {'n_neighbors':np.arange(1,50)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_uuid": "f47376c83b43e5b724410de51a9657934e9c4b20"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn_cv= GridSearchCV(knn,param_grid,cv=5)\n",
    "knn_cv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_uuid": "01a0c192bc77da41c61b2d9161fc052210542cfc"
   },
   "outputs": [],
   "source": [
    "knn_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "54eea8ab157f3d67a725422deb303a082ff695b5"
   },
   "outputs": [],
   "source": [
    "knn_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35a9ac99ade7f0e392272be0a021747aeb4759ac"
   },
   "source": [
    "Thus a knn classifier with number of neighbors as 14 achieves the best score/accuracy of 0.7578 i.e about 76%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
